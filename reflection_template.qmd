---
title: "STAT 331 Portfolio"
author: "Stella Raymond!"
format: 
  html: 
    self-contained: true
layout: margin-left
editor: visual
execute: 
  eval: false
  echo: true
  message: false
---

[**My Grade:**]{.underline} I believe my grade equivalent to course work evidenced below to be an A.

(I really should have taken this class as graded. I currently have it as credit/no credit. I just didn't want to stress over it but I don't think c/nc changed that in hindsight).

[**Learning Objective Evidence:**]{.underline} In the code chunks below, provide code from Lab or Challenge assignments where you believe you have demonstrated proficiency with the specified learning target. Be sure to specify **where** the code came from (e.g., Lab 4 Question 2).

## Working with Data

**WD-1: I can import data from a *variety* of formats (e.g., csv, xlsx, txt, etc.).**

-   `csv`

```{r}
#| label: wd-1-csv
#This originated from Lab 2 Challenge (the first load in), the very first load in. I removed library(viridis) becuase that is not relevant here.
library("here")
surveys <- read_csv(here::here("data","surveys.csv"))
```

-   `xlsx`

```{r}
#| label: wd-1-xlsx
#This code originates from the practice activity 4 - Military spending. Here, not only do we load in a xlsx file, we also specify the sheet and rows we wanted to include. 
military <- read_xlsx(here::here("data", 
                                 "gov_spending_per_capita.xlsx"), 
                      sheet = "Share of Govt. spending", 
                      skip  = 7, 
                      n_max = 190)
```

-   `txt`

```{r}
#| label: wd-1-txt
#This code originates from Check-in 2.3. 
ages_tab <-Â read_table(file = here::here("Week 2", 
                                         "Check-ins", 
                                         "Ages_Data", 
                                         "ages_tab.txt"))
```

**WD-2: I can select necessary columns from a dataset.**

```{r}
#| label: wd-2
#This originated from Lab 4 Q4. Here I select only region, study_year, and mhi_2018 to remain within the dataset)
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

**WD-3: I can filter rows from a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   numeric

```{r}
#| label: wd-3-numeric
#This originated from Lab 4 Q4. Here I filter out year 2008 and 2018.
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

-   character -- specifically a string (example must use functions from **stringr**)

```{r}
#| label: wd-3-string
#This question originates from Lab 5. In this particular chunk, I filter using str_detect so I could extract all Annabels
person |> 
  full_join(interview,
            join_by(id == person_id))|>
  filter(
    (address_street_name == "Northwestern Dr" & address_number == max(address_number)) |
    (address_street_name == "Franklin Ave" & str_detect(name, "^Annabel"))
         )|>
    pull(transcript,
         name)
```

-   factor

```{r}
#| label: wd-3-factor
#This is a line of code from lab 9 question 1 where I filter the correctly paired families filter(paired == "TRUE") in order to count up the total correctly randomized families (absent of FALSE values).

randomBabies <- function(nBabies = 4){
  baby_tib <- tibble(pnum  = 1:nBabies,
                     bbnum = sample(1:nBabies, 
                                      size = nBabies, 
                                      replace = FALSE)) |>
  mutate(paired = if_else((pnum == bbnum), 
                          "TRUE", 
                          "FALSE")) |>
  filter(paired == "TRUE") |>
  nrow()
  return(baby_tib)}

```

-   date (example must use functions from **lubridate**)

```{r}
#| label: wd-3-date
#This code comes from Lab 5. Originally, I did not convert the dates into lubridate dates (or even utilize the dates as an important means of finding the person). However, I recently looked back and saw that Miranda went to the symphony *specifically* in december 2017. I revisited this code and filtered the year 2017 and the month 12 to ensure that she was the culprit. (code near the end, by the way)
inner_join(person,
           drivers_license,
           join_by(license_id == id)) |>
  inner_join(facebook_event_checkin,
           join_by(id == person_id)) |>
  inner_join(income, join_by(ssn == ssn)) |>
  mutate(comparative_income = mean(annual_income),
         date = ymd(date)) |>
  group_by(id, event_name) |>
  mutate(attendance = n()) |>
  ungroup()|>
  select(id,
         hair_color,
         name,
         height,
         plate_number,
         car_make,
         car_model,
         attendance,
         ssn,
         annual_income,
         comparative_income,
         date) |>
  filter(year(date) == 2017,
         month(date) == 12,
         hair_color == "red",
         height == c(65:67),
         car_make == "Tesla",
         car_model == "Model S",
         attendance == 3,
         annual_income > comparative_income) |>
  slice(1)|>
  pull(name,
       id)
```

**WD-4: I can modify existing variables and create new variables in a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   numeric (using `as.numeric()` is not sufficient)

```{r}
#| label: wd-4-numeric
#This originates from the set up in Lab 3 Challenge (the first chunk). Here, I am creating a cleaner version of the dataset's question_no variable by subtracting 900 (effectively converting each question number into the a number 1-9 isntead of 901-909).
clean_evals <- evals |>
  mutate(sex = gender,
         teacher_id = as.factor(teacher_id),
         question_no = question_no-900)
```

-   character -- specifically a string (example must use functions from **stringr**)

```{r}
#| label: wd-4-string
#This code originates from Lab 9 question 8. It is my submitted code but I realized that I could do better. I could automate the alterations by using stringr. (see beneath this line of code).
all_simulations |>
  mutate(n = as.character(n), 
         n = fct_recode(n, 
             `10 Simulations` = "10", 
             `100 Simulations` = "100", 
             `1000 Simulations` = "1000", 
             `10000 Simulations` = "10000"))

#I quickly realised that this was not as efficient as it could/should be. I loaded in the stringr package and altered the code such that it added " Simulations" to the end. 

all_simulations |>
  mutate(n = str_glue("{n} Simulations"),
         n = as.character(n))

```

-   factor (example must use functions from **forcats**)

```{r}
#| label: wd-4-factor
#The question is from Lab 4 Q6. In this code chunk, I am changing age into a factor that includes all three age groups. Afterwards, I recode it and change the names.
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool"))


#Additionally, I demonstrated my ability to modify factors in a more recent lab. From Lab 9, i change the names of the factors by using fct_recode. However! upon looking back, I do no approve of this code and wanted to incorperate it to show I can efficiently alter code!
all_simulations |>
  mutate(n = as.character(n), 
         n = fct_recode(n, 
             `10 Simulations` = "10", 
             `100 Simulations` = "100", 
             `1000 Simulations` = "1000", 
             `10000 Simulations` = "10000"))

#I then realized that I could have done a lot better in regards to my efficiency. I changed it to the following code. Eventhough it is not a fct_recode, I still wanted to include it in this section to show I know how to use multiple methods and identify areas of efficiency improvement
all_simulations |>
  mutate(n = str_glue("{n} Simulations"),
         n = as.character(n))
```

-   date (example must use functions from **lubridate**)

```{r}
#| label: wd-4-date
#As mentioned in WD-3, this line of code comes from Lab 5 and is altered from my original submission. Here is an alternative fix to my WD-3 date fix. Here, I first use lubridate to convert my date into ymd and use these dates create new year and month columns. I modify the ymd dates and extract the data. This outputs the same result as the other line of code but just uses a slightly different method.

inner_join(person,
           drivers_license,
           join_by(license_id == id)) |>
  inner_join(facebook_event_checkin,
           join_by(id == person_id)) |>
  inner_join(income, join_by(ssn == ssn)) |>
  mutate(comparative_income = mean(annual_income),
         date = ymd(date),
         year = year(date),
         month = month(date)) |>
  group_by(id, event_name) |>
  mutate(attendance = n()) |>
  ungroup()|>
  select(id,
         hair_color,
         name,
         height,
         plate_number,
         car_make,
         car_model,
         attendance,
         ssn,
         annual_income,
         comparative_income,
         year,
         month) |>
  filter(year == 2017,
         month == 12,
         hair_color == "red",
         height == c(65:67),
         car_make == "Tesla",
         car_model == "Model S",
         attendance == 3,
         annual_income > comparative_income) |>
  slice(1)|>
  pull(name,
       id)
```

**WD-5: I can use mutating joins to combine multiple dataframes.**

-   `left_join()`

```{r}
#| label: wd-5-left
#This code is from Lab 4 question 2. Here I implement a left_join()

ca_childcare <- counties |>
  filter(state_abbreviation == "CA") |>
  left_join(childcare_costs,
            by = join_by(county_fips_code == county_fips_code))
```

-   `right_join()`

```{r}
#| label: wd-5-right
#This is from Lab 5, a portion of the lab I am currently revising. Here, I confirm that the suspect is the true murderer by joining the two dataframes and pulling her name.
#I wanted to use a right join here because an interjoin would remove the NA. Here, I was checking if Miranda had an interview or not and wanted an output of either the interview trascript or an NA
right_join(interview,
           person,
           join_by(person_id == id))|>
  select(name, 
         transcript)|>
  filter(str_detect(name, "Miranda Priestly"))
```

-   `inner_join()`

```{r}
#| label: wd-5-inner
#Here i include code from Lab 5 and combine three different dataframes. I also use robust & resitant coding by confirming that ssn == ssn just in case. 
inner_join(person,
           drivers_license,
           join_by(license_id == id)) |>
  inner_join(facebook_event_checkin,
           join_by(id == person_id)) |>
  inner_join(income, join_by(ssn == ssn))
```

-   `full_join()`

```{r}
#| label: wd-5-full-not-needed
#This code originates from lab 5 where I have altered my original code. Here, I utilize a full_join() to incorpreate the data from the interview dataframe in order to go straight from the 2 witnesses to the transcripts.
person |> 
  full_join(interview,
            join_by(id == person_id))|>
  filter(
    (address_street_name == "Northwestern Dr" & address_number == max(address_number)) |
    (address_street_name == "Franklin Ave" & str_detect(name, "Annabel"))
         )|>
    pull(transcript,
         name)

#This was deleted from requirements!*
```

**WD-6: I can use filtering joins to filter rows from a dataframe.**

-   `semi_join()`

```{r}
#| label: wd-6-semi
#This code is from lab 5 and is a part of the revision I am making. Here, I utilize a semi_join near the end of my code chunk
gym_names <- inner_join(get_fit_now_member, 
                        get_fit_now_check_in,
                        by = join_by(id == membership_id)
                        )|>
  select(name,
         id,
         membership_status,
         check_in_date
         )|>
  filter(str_detect(id, "^48Z"),
         membership_status == "gold")

inner_join(person,
           drivers_license,
           join_by(license_id == id)
           ) |>
  filter(str_detect(
         plate_number, "H42W"))|>
  select(name,
         id
         ) |>
  semi_join(gym_names,
            join_by(name == name)
            )|>
  inner_join(interview,
            join_by(id == person_id)
            )|>
  pull(transcript)
```

-   `anti_join()`

```{r}
#| label: wd-6-anti
#This code also originates from Lab 5 and it is the very last code chunk confirming who the final suspect is. I created this code after submission, atering it from my original code. Let me walk you thought process. 
#I used an anti join between the person and interview. This only keeps the people who are NOT included in the interview dataset. I then specifically look for Miranda. If she did have an interview, her name would not be present. If she DIDN'T have an interview, her name would show up. So, if she is suspiciously without an interview, it would return true (and false if not).

suspect <- anti_join(person,
                     interview,
                     join_by(id == person_id))|>
  select(name)|>
  filter(str_detect(name, "Miranda Priestly"))

  if (nrow(suspect) > 0) 
     {print("Suspicious = TRUE")
      } else{print("Suspicious = FALSE")
      }
```

**WD-7: I can pivot dataframes from long to wide and visa versa**

-   `pivot_longer()`

```{r}
#| label: wd-7-long
#This code originates from a small portion of Lab 4's Question 6. Here I take the three separate age colomns and combine them into a single colomn.
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price")
```

-   `pivot_wider()`

```{r}
#| label: wd-7-wide
#This is from Lab 4 question 4. Here, I make two new colomns out of the two years from study_year.
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

## Reproducibility

**R-1: I can create professional looking, reproducible analyses using RStudio projects, Quarto documents, and the here package.**

I've done this in the following provided assignments:

-   Lab 3 Challenge

-   Lab 4

-   Lab 5

-   Lab 8 & Lab 9 from Challenge 9

**R-2: I can write well documented and tidy code.**

-   Example of **ggplot2** plotting

```{r}
#| label: r-2-1
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool"))|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7))
```

-   Example of **dplyr** pipeline

```{r}
#| label: r-2-2
#This code chunk is from Lab 4 question 3. I chose to include this code chunk because it was comprised of a large amount of characters and was thus difficult to deal with. Nevertheless, I was able to keep the code tidy and organized.
ca_childcare <- ca_childcare |> 
  mutate(county_name = str_remove(county_name, 
                                  " County"),
         region = fct_collapse(.f = county_name,
                               "Superior California" = c("Butte", 
                                                         "Colusa", 
                                                         "El Dorado", 
                                                         "Glenn", 
                                                         "Lassen", 
                                                         "Modoc", "Nevada", 
                                                         "Plumas", 
                                                         "Sacramento", 
                                                         "Shasta", 
                                                         "Sierra", 
                                                         "Siskiyou", 
                                                         "Sutter", 
                                                         "Tehama", 
                                                         "Yolo", 
                                                         "Yuba", 
                                                         "Placer"),
                               "San Francisco Bay Area" = c("Alameda", 
                                                            "Contra Costa", 
                                                            "Marin", 
                                                            "San Francisco", 
                                                            "San Mateo", 
                                                            "Santa Clara", 
                                                            "Solano"),
                               "North Coast" = c("Del Norte", 
                                                 "Humboldt", 
                                                 "Lake", 
                                                 "Mendocino", 
                                                 "Napa", 
                                                 "Sonoma", 
                                                 "Trinity"),
                               "Orange County" = c("Orange"),
                               "Central Coast" = c("Monterey", 
                                                   "San Benito", 
                                                   "San Luis Obispo", 
                                                   "Santa Barbara", 
                                                   "Santa Cruz", 
                                                   "Ventura"),
                               "Northern San Joaquin Valley" = c("Alpine", 
                                                                 "Amador", 
                                                                 "Calaveras", 
                                                                 "Madera", 
                                                                 "Mariposa", 
                                                                 "Merced", 
                                                                 "Mono", 
                                                                 "San Joaquin", 
                                                                 "Stanislaus", 
                                                                 "Tuolumne"),
                               "Los Angeles County" = c("Los Angeles"),
                               "Southern San Joaquin Valley" = c("Fresno", 
                                                                 "Kern", 
                                                                 "Kings", 
                                                                 "Inyo", 
                                                                 "Tulare"),
                               "Inland Empire" = c("Riverside", 
                                                   "San Bernardino"),
                               "San Diego Imperial" = c("San Diego", 
                                                       "Imperial")))
```

-   Example of function formatting

```{r}
#| label: r-2-3
#Here is a line of code from Lab 8 question 4/5 after my revision. Here, I format a function such that it has multiple function stops and efficiency (by using ranging). 

rescale_01 <- function(x, na.rm = TRUE){
  if(!is.numeric(x) | length(x) <= 1)
  {stop("change input")}
  ranging <- range(x, 
                   na.rm = na.rm)
  return((x - ranging[1]) / 
           (ranging[2] - ranging[1]))}

#I also create another function in this lab (Question 8) seen below. It is properly formatted and has function stops (after revisions).

rescale_column <- function(df, name){
  if(!is.data.frame(df))
  {stop("change input")}
  df |> 
    mutate(across(.cols = {{name}},
                  .fns = ~rescale_01(.x)))
  }
```

**R-3: I can write robust programs that are resistant to changes in inputs.**

-   Example -- any context

```{r}
#| label: r-3-example
#Here is a line of code from Lab 4 question 5. In this, i utilize a robust means of extracting the minimum median region value using slice_min. Instead of selecting a certain row, i request that the minimum value is extracted, ensuring that regardless of how shuffled the dataset is, i get the exact value i want. 
ca_childcare |>
  filter(study_year == "2018") |>
  select(region, 
         study_year, 
         mcsa) |>
  group_by(region)|>
  summarise(median_region = median(mcsa))|>
  slice_min(median_region)

#Additionally, I also use summarise in the context of multiple columns (not just specificly named columns). This code originates from Lab 7.
fish |>
  summarize(across(.cols = trip:species,
                   .fns = ~sum(is.na(.x)
                               )
                   )
            )
```

-   Example of function stops

```{r}
#| label: r-3-function-stops
#This code originates from lab 7 and it is a post-revision line of code. Here I use a function stop to ensure that the input is both numeric and ensure that the input is not greater than 1. 

rescale_01 <- function(x, na.rm = TRUE){
  if(!is.numeric(x) | length(x) <= 1)
  {stop("change input")}
  ranging <- range(x, 
                   na.rm = na.rm)
  return((x - ranging[1]) / 
           (ranging[2] - ranging[1]))}
```

## Data Visualization & Summarization

**DVS-1: I can create visualizations for a *variety* of variable types (e.g., numeric, character, factor, date)**

-   at least two numeric variables

```{r}
#| label: dvs-1-num
#This code is from Lab 4 question 7 and I like how I added the colors and an extra variable (region) just to see how it would look, i thought that was pretty cool. I also added dollar signs to both axes just now.
ca_childcare |>
  filter(study_year == "2018") |>
  select(mhi_2018,
         mc_infant,
         study_year,
         region) |>
  ggplot(mapping = aes(x = mhi_2018, 
                       y = mc_infant,
                       color = region)
         )+
  geom_jitter()+
  geom_smooth(method = lm, 
              color = "green4")+
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_dollar()) +
  theme_bw()+
  labs(title = "Household Income and Weekly Price of 2018 Center-Based Infant Childcare",
       x = "Median Weekly Household Income",
       y = "",
       subtitle = "Y-axis representing Full-Time Median Weekly Price",
       color = "California Region")

#This code originates from lab 7 Q7 and compares the original to the rescaled plot. I include this because i am proud of how similar I made the graphs.
fish |>  
  ggplot(aes(x = length)) + 
  geom_histogram(binwidth = 45) +
  labs(x = "Original Values of Fish Length (mm)",
       y = "",
       subtitle = "y-axis representing Count",
       title = "Blackfoot River Trout Lengths") +
  scale_y_continuous(limits = c(0,4000)) +
  theme_bw(base_family = "sans")
# Code for Q7 plot.
fish |>  
  mutate(length_scaled = rescale_01(length))|>
  ggplot(aes(x = length_scaled)) + 
  geom_histogram(binwidth = .043) +
  labs(x = "Scaled Values of Fish Length (mm)",
       y = "",
       subtitle = "y-axis representing Count",
       title = "Blackfoot River Trout Lengths") +
  scale_y_continuous(limits = c(0,4000)) +
  theme_bw(base_family = "sans")

#This code originates from Lab 9 question 3. Here, I visualize the number of babies correctly returned.
#as mentioned, font change help by means of the help bar in RStudio
baby_table |>
  ggplot(mapping = aes(x = ncorrect, 
                       y = proportion)) +  
  geom_bar(stat = "identity") +
  labs(title = "Percentage of Correctly Paired Families",
       y = "",
       subtitle = "Sample of 4 Babies  Under Random Conditions",
       x = "Correct Babies Returned") +
  scale_y_continuous(labels = label_percent()) +
  theme_bw(base_family = "sans", 
           base_size = 11)

```

-   at least one numeric variable and one categorical variable

```{r}
#| label: dvs-2-num-cat
#This code originates from my Lab 2 challenge (the third chunk). (Here I deleted an extra comma but other than that, should be good). Numeric: wight Cat: species
ggplot(data = surveys,
       mapping = aes(x = weight, 
                     . = species)) +
  geom_jitter(alpha = 0.5, 
              color = "green4") +
  geom_density_ridges(alpha = 0.5) +
  labs(x="Weight (g)", 
       y="", 
       title= "Relationship Between Rodent Weight and Hindfoot Length",
       subtitle= "Y-axis representing Species")
```

-   at least two categorical variables

```{r}
#| label: dvs-2-cat
#This is from Lab 2 Challenge (the 4th and 5th chunks). Here I create a graph using the vector color pallete "Palette_Earth" I created. The two categorical variables are Genus and Species. 

Palette_Earth <- c("#a0eb67", "#83ba59", "#3e6b1b", "#78eb05", "#ffd673", "#c99f3a", "#8a6204", "#d9cb0f")

surveys |> 
  ggplot(aes(x = weight, 
             y = species, 
             color = genus)) +
  geom_boxplot() +
  scale_color_manual(values = Palette_Earth) +
  labs(x = "Weight (g)", 
       y = "", 
       subtitle = "Species", 
       legend = "Genus")

#I also added this graph as a second example. This one is from challenege 3 Q2 and the two cat variables are SET_level and sen_level. I like the annotations I added

teacher_evals_compare |>
ggplot(mapping=aes(x = sen_level, 
                   fill = SET_level)) +
  
  theme_bw()+
  scale_fill_manual(values = c("#2d73b5",
                               "#c97a0a")) +
  geom_bar(show.legend = FALSE) +
  annotate("text", 
           y = 50,
           x = "senior", 
           label = "Standard", 
           color = "white") +
  annotate("text", 
           y = 250, 
           x = "senior", 
           label = "Excellent", 
           color = "white") +
  annotate("text", 
           y = 25, 
           x = "junior", 
           label = "Standard", 
           color = "white") +
  annotate("text", 
           y = 175, 
           x = "junior", 
           label = "Excellent", 
           color = "white") +
  labs(title = "Evaluation of In-Class Activity Use by Instructor Seniority",
       x="Seniority of Instructor",
       y="",
       subtitle = "Number of Sections",
       fill = "SET Level")
#recently added show.legend = FALSE after you suggested to remove the legend*

```

-   dates (timeseries plot)

```{r}
#| label: dvs-2-date

#This plot is from lab 4 question 6. I do a lot of adjustments to make sure it all fit. I also added nice labels, coloring, and size changes. The time series is in years
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool")
         )|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()
                     ) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7))
```

**DVS-2: I use plot modifications to make my visualization clear to the reader.**

-   I can ensure people don't tilt their head

```{r}
#| label: dvs-2-1
#Here is a line of code from Lab 2 question 16. I implement two ways of increasing readiblity: altering species to the y-axis and using the subtitle in place of a y-axis label.
ggplot(data = surveys,
       mapping = aes(
         x = weight, 
         y = species
         )
       ) +
  geom_jitter(alpha = 0.5, 
              color = "green4")+
  geom_boxplot(outliers = FALSE)+
  labs(
    x = "Weight (g)", 
    y = "", 
    title = "Relationship Between Rodent Weight and Hindfoot Length",
    subtitle = "Y-axis representing Species",)
```

-   I can modify the text in my plot to be more readable

```{r}
#| label: dvs-2-2
#This is from lab 4 question 6. Here, I make the plot more readible through various means. In particular, I alter the size of the axes, the label names, and the order of the legend.
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool")
         )|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()
                     ) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7)) 
```

-   I can reorder my legend to align with the colors in my plot

```{r}
#| label: dvs-2-3
#Again, this code is from lab 4 question 6. Here, I reorder the legend color data in order to match with the lines
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool")
         )|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()
                     ) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7)) 
```

**DVS-3: I show creativity in my visualizations**

-   I can use non-standard colors

```{r}
#| label: dvs-3-1
#I used non standard colors in Lab 2 challenge (7th and 8th chunks) by using colors from packages ggsci and viridis
surveys |> 
  ggplot(aes(x = weight, y = species, color = genus)
         ) +
  geom_boxplot() +
  scale_colour_viridis_d() +
  labs(x = "Weight (g)", y = "", subtitle = "Species", legend = "Genus")

#this second code chunk is for a separate graph with alternative colors
surveys |> 
  ggplot(aes(x = weight, y = species, color = genus)
         ) +
  geom_boxplot() +
  scale_color_lancet() +
  labs(x = "Weight (g)", y = "", subtitle = "Species", legend = "Genus")
```

-   I can use annotations

```{r}
#| label: dvs-3-2
#This code is from lab 3 challenge (question 2) and I have updated it such that I included annotations in the graph to better visualize which section of the stacked bar graph is which. This was not included in the revisions but added here.
teacher_evals_compare |>
ggplot(mapping=aes(x = sen_level, 
                   fill = SET_level)) +
  
  theme_bw()+
  scale_fill_manual(values = c("#2d73b5",
                               "#c97a0a")) +
  geom_bar(show.legend = FALSE) +
  annotate("text", 
           y = 50,
           x = "senior", 
           label = "Standard", 
           color = "white") +
  annotate("text", 
           y = 250, 
           x = "senior", 
           label = "Excellent", 
           color = "white") +
  annotate("text", 
           y = 25, 
           x = "junior", 
           label = "Standard", 
           color = "white") +
  annotate("text", 
           y = 175, 
           x = "junior", 
           label = "Excellent", 
           color = "white") +
  labs(title = "Evaluation of In-Class Activity Use by Instructor Seniority",
       x="Seniority of Instructor",
       y="",
       subtitle = "Number of Sections",
       fill = "SET Level")
#recently added show.legend = FALSE after you suggested to remove the legend*
```

-   I can be creative...

```{r}
#| label: dvs-3-3
#This code originates from Lab 4 Question 7. I will admit, I did not follow the question completely: I extended past what was asked for and included regional data when i wasn't supposed to. I would like to argue that this shows my creativity. I was originally just playing around with the graph and wanted to explore some trends. I was curious how region played into this correlation and was interested in the results of this inclusion (for examoke, all San Fransisco values are very high, Central Coast are right in the middle, etc.). For this reason, adding color just as an extra layer to graph demonstrates that I am thinking about what researchers would want to present and creative ways to include notible finds. 
ca_childcare |>
  filter(study_year == "2018") |>
  select(mhi_2018,
         mc_infant,
         study_year,
         region) |>
  ggplot(mapping = aes(x = mhi_2018, 
                       y = mc_infant,
                       color = region)
         )+
  geom_jitter()+
  geom_smooth(method = lm, 
              color = "green4")+
  theme_bw()+
  labs(title = "Household Income and Weekly Price of 2018 Center-Based Infant Childcare",
       x = "Median Weekly Household Income ($)",
       y = "",
       subtitle = "Y-axis representing Full-Time Median Weekly Price ($)",
       color = "California Region")

#Additionally, I chose to show my creativity in my Lab 9 challenge (Lab 9 question 2) when changing up the tables. As a result, I added percentages, changed color to green (my favorite color I commonly use), utilized subtitle to specify my table, and added table striping. 

baby_table |>
  pivot_wider(values_from = proportion,
              names_from = ncorrect) |>
  gt() |>
  fmt_percent() |>
  tab_header(
    title = "Correctly Paired Families Under Random Conditions",
    subtitle = "Sample Size of 4 Babies") |>
  opt_stylize(color = "green", 
              add_row_striping = TRUE)

#Additionally, in the same lab (lab 9 question 3), I created this visualization. I changed the y-axis into percentages, utilized theme_bw to alter the font and size, and created descriptive titles.

baby_table |>
  ggplot(mapping = aes(x = ncorrect, 
                       y = proportion)) +  
  geom_bar(stat = "identity") +
  labs(title = "Percentage of Correctly Paired Families",
       y = "",
       subtitle = "Sample of 4 Babies  Under Random Conditions",
       x = "Correct Babies Returned")+
  scale_y_continuous(labels = label_percent())+
  theme_bw(base_family = "sans", 
           base_size = 11)

```

**DVS-4: I can calculate numerical summaries of variables.**

-   Example using `summarize()`

```{r}
#| label: dvs-4-summarize
#Here is an example from Lab 4 question 4 where I utilize summarise to create a new median_region variable. 
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
#fixed the grouping after midterm portfolio suggestion

#Additionally, I also use summarise in the context of multiple columns. This code originates from Lab 7, question 1.
fish |>
  summarize(across(.cols = trip:species,
                   .fns = ~sum(is.na(.x)
                               )
                   )
            )
```

-   Example using `across()`

```{r}
#| label: dvs-4-across
# Here is some code originating from lab 7 question 8. Here I use across within a function with a dataframe check (of which, i added with revisions and with the help of the R help bar). Here, I am using across to look at the name input and uses my other function rescale_01 to rescale the dataframe values. I also removed unnecessary quotation marks after submission in my revision.

rescale_column <- function(df, name){
  if(!is.data.frame(df))
  {stop("change input")}
  df |> 
  mutate(across(.cols = {{name}},
                .fns = ~rescale_01(.x)))
  }
```

**DVS-5: I can find summaries of variables across multiple groups.**

-   Example 1

```{r}
#| label: dvs-5-1
#The code originates from Lab 4 question 4. Here, I am able to summarize the median region household income based on 2018 values. In this chunk, i group by the two groups region and study_year.
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()

#Additionally, I use summarise in the context of multiple columns by use of across(). This code originates from Lab 7.
fish |>
  summarize(across(.cols = trip:species,
                   .fns = ~sum(is.na(.x)
                               )
                   )
            )
```

-   Example 2

```{r}
#| label: dvs-5-2
#Additionally, I've used the group_by() function to creatue an attendance variable by utilizing the event_name and id variables. This code originates from my updated Lab 5 revisions.

inner_join(person,
           drivers_license,
           join_by(license_id == id)) |>
  inner_join(facebook_event_checkin,
           join_by(id == person_id)) |>
  inner_join(income, join_by(ssn == ssn)) |>
  mutate(comparative_income = mean(annual_income)) |>
  group_by(id, event_name) |>
  mutate(attendance = n()) |>
  ungroup()
```

**DVS-6: I can create tables which make my summaries clear to the reader.**

-   Example 1

```{r}
#| label: dvs-6-1
#This is from Lab 4. I actually fixed this piece of code to better explain my data. Originally I had "mhi_in" as a prefix but altered it to "median_household_income_" to better explain the data.
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "median_household_income_")|>
  ungroup()

#I changed SET_level -> SET_Q3_score_average and sen_level -> seniority_level

#Below, I've also created a table to display the simulated baby returns in Lab 9 (challenge). Additionally, I create striping, titles and subtitles, and converted proportions into percentages for easier reading

baby_table |>
  pivot_wider(values_from = proportion,
              names_from = ncorrect) |>
  gt() |>
  fmt_percent() |>
  tab_header(
    title = "Correctly Paired Families Under Random Conditions",
    subtitle = "Sample Size of 4 Babies") |>
  opt_stylize(color = "green", 
              add_row_striping = TRUE)
#After recieving feedback, I pivoted the table wider and made the title a bit more detailed

```

-   Example 2

```{r}
#| label: dvs-6-2
#This code originates from Lab 9 challenge (so, reformated Lab 8). Here, I utilize gt() to reformat the table and create informative titles/subtitles. I also retroactively added a cool new color to the title and subtitle. Here, the count for each of the columns are clearly aligned on the table and striped for easier viewing.
tibble(Parameters = names(fish), 
       Count = map_int(.x = fish,
                       .f = ~sum(is.na(.x))
                       ))|> 
  gt()|>
  tab_header(title = md("Blackfoot River Trout Absent Values"),
             subtitle = md("Rainbow, Westslope Cutthroat, Bull, & Brown trout")
             )|>
  tab_style(style = cell_fill(color = "darkseagreen3"),
            locations = cells_title(groups = c("title"))
            )|>
  tab_style(style = cell_fill(color = "darkseagreen1"),
            locations = cells_title(groups = c("subtitle"))
            )|>
  opt_stylize(color = "green", 
              add_row_striping = TRUE)
```

**DVS-7: I show creativity in my tables.**

-   Example 1

```{r}
#| label: dvs-7-1
#This code is from challenge 9 (but originally lab 8 Q1). Here, I use unusual colors (i liked them because they were not too stand out but still showed a subtile difference). I also use gt() to change the title/subtitle.

tibble("Column Name" = colnames(surveys),
       "Value Type" = map_chr(.x = surveys, 
                              .f = typeof)) |>
  bind_cols() |>
  gt() |>
  tab_header(title = "Column and Value Types",
             subtitle = "From BlackfootFish Dataset")|>
  tab_style(style = cell_fill(color = "cornsilk"),
            locations = cells_title(groups = c("subtitle"))) |>
  tab_style(style = cell_fill(color = "cornsilk3"),
            locations = cells_title(groups = c("title")))
```

-   Example 2

```{r}
#| label: dvs-7-2
#Here is a line of code from Lab 9 Challenge (lab 8, question 4 (originally from lab 7)). I show my creativity. This code combines my original code, revisions after input, and my recent additionas. As mentioned, I am a fan of green. Here, I watned to use different colors/shades of green that I liked to emphasize the titles. I also added striping which helped viewing. I also showed my creativity by placing the species names in the subtitle, making it organized and pretty. 
tibble(Parameters = names(fish), 
       Count = map_int(.x = fish,
                       .f = ~sum(is.na(.x))
                       )
       )|> 
  gt()|>
  tab_header(title = md("Blackfoot River Trout Absent Values"),
             subtitle = md("Rainbow, Westslope Cutthroat, Bull, & Brown trout")
             )|>
  tab_style(style = cell_fill(color = "darkseagreen3"),
            locations = cells_title(groups = c("title"))
            )|>
  tab_style(style = cell_fill(color = "darkseagreen1"),
            locations = cells_title(groups = c("subtitle"))
            )|>
  opt_stylize(color = "green", 
              add_row_striping = TRUE)
```

## Program Efficiency

**PE-1: I can write concise code which does not repeat itself.**

-   using a single function call with multiple inputs (rather than multiple function calls)

```{r}
#| label: pe-1-one-call
#Here is a code chunk from Lab 3 Challenge set up. Here, I use one single mutate function with three inputs (sex, teacher_id, and question_no changes) instead of three separate mutate functions. 
clean_evals <- evals |>
  mutate(sex = gender,
         teacher_id = as.factor(teacher_id),
         question_no = question_no-900)

#I also use efficiency by using one function to do the work of one. For example, in this code chunk from Lab 8 question 3, I pivot twice then use the select() function to both select the columns I want as well as rename the columns (instead of using select then rename). I am especially proud of my ability to replicate this table without asking for assistance. 
evals |>
  mutate(seniority_level = if_else(seniority <= 4,
                             "junior",
                             "senior")
         ) |>
  select(sex, 
         seniority_level, 
         academic_degree,
         teacher_id) |>
  distinct(teacher_id,
           .keep_all = TRUE) |>
  pivot_longer(cols = sex:academic_degree,
               names_to = "columns",
               values_to = "values") |>
  count(values) |>
  pivot_wider(names_from = values,
               values_from = n) |>
  bind_cols() |>
#The rest is just for visuals
  select(Female = female,
         Male = male,
        "Junior (4 years or less)" = junior,
        "Senior (4 years or more)" = senior,
        "No Degree" = no_dgr,
         Masters = ma,
         Doctorate = dr,
         Professor = prof)|>
  kable() |> 
  kable_styling(bootstrap_options = "striped") 
#alternations made to this code after getting your advice. 

#Lastly, I use the names_glue() function to alter the names of the new pivoted columns automatically. This is from Lab 9 question 7. This prevents the need to use the rename() function. 

all_simulations |>
  select(!df) |>
  unnest(simulated_m) |>
  pivot_wider(names_from = n, 
              values_from = simulated_m,
              names_glue = "{n} simulations",
              values_fn = mean) 

```

-   `across()`

```{r}
#| label: pe-1-across
#This code comes from Lab 7 question 8 (after revisions). Here, I utilize mutate(across()) within a function to automate my rescale_01() function such that I can use it for an inputted dataframe. 
rescale_column <- function(df, name){
  if(!is.data.frame(df))
  {stop("change input")}
  df |> 
    mutate(across(.cols = {{name}},
                  .fns = ~rescale_01(.x))) #.fns added for proper formatting
  }
```

-   `map()` functions

```{r}
#| label: pe-1-map-1

#This code originates from Lab 9 Question 6. Here, I utilize pmap() to analyze probability. 
all_simulations <- grid |> 
  mutate(simulated_m = pmap(.l = list(n = n,
                                      df = df), 
                            .f = simulate_means)) |> 
  unnest(simulated_m)

#This code also originates from Lab 9 but from Question 1. Here, I utilize map_int to simulate 10,000 simulations from the randomBabies code

randomBabies <- function(nBabies = 4){
  baby_tib <- tibble(pnum  = 1:nBabies,
                     bbnum = sample(1:nBabies, 
                                      size = nBabies, 
                                      replace = FALSE)) |>
  mutate(paired = if_else((pnum == bbnum), 
                          "TRUE", 
                          "FALSE")) |>
  filter(paired == "TRUE") |>
  nrow()
  return(baby_tib)}

results <- map_int(.x = 1:10000,
                   .f = ~randomBabies(nBabies = 4))

#Lastly, this code is from Lab 8 where I revised lab 3 in question 2. Here, i use map_at() to identify the specfic columns I wanted to alter using as.factor(). This increased efficiency (instead of just using as.factor() on each individual column).

evals |>
  head() |> #so you don't have a massive output
  map_at(.at = c("teacher_id", 
               "weekday", 
               "academic_degree", 
               "seniority",
               "gender"),
         .f = ~as.factor(.x))|>
  bind_cols()|>
  kable()|>
  kable_classic(lightable_options = "hover")
```

**PE-2: I can write functions to reduce repetition in my code.**

-   Function that operates on vectors

```{r}
#| label: pe-2-1
#This is from Lab 7 question 4/5. Here I create a function that takes a single vector (and has a check for that vector).
rescale_01 <- function(x, na.rm = TRUE){
  if(!is.numeric(x) | length(x) <= 1)
  {stop("change input")}
  ranging <- range(x, 
                   na.rm = na.rm)
  return((x - ranging[1]) / 
           (ranging[2] - ranging[1]))}


```

-   Function that operates on data frames

```{r}
#| label: pe-2-2
#This code is from Lab 7, question 8. Here, I create a function that takes a inputted dataframe "df" and rescales the specific name you give it with the rescale_01() function. 

rescale_column <- function(df, name){
  if(!is.data.frame(df))
  {stop("change input")}
  df |> 
    mutate(across(.cols = {{name}},
                  .fns = ~rescale_01(.x)))
    }

fish |>
  rescale_column(name = c(weight,
                          length))
```

**PE-3: I can use iteration to reduce repetition in my code.**

-   `across()`

```{r}
#| label: pe-3-across
#Again, this code originates from lab question 8. Here, I use across such that I can rescale_01() multiple columns at once instead of repeating the same code. This increases efficiency and allows me to write a single line instead of a lot of lines

function(df, name){
  if(!is.data.frame(df))
  {stop("change input")}
  df |> 
    mutate(across(.cols = {{name}},
                  .fns = ~rescale_01(.x)))
    }
```

-   `map()` function with **one** input (e.g., `map()`, `map_chr()`, `map_dbl()`, etc.)

```{r}
#| label: pe-3-map-1
#This code is from lab 8 Q4 where I find the summaries across the dataframe fish (a single input). 
tibble(Parameters = names(fish), 
       Count = map_int(.x = fish,
                       .f = ~sum(is.na(.x))))|> 
  gt()|>
  tab_header(title = md("Blackfoot River Trout Absent Values"),
             subtitle = md("Rainbow, Westslope Cutthroat, Bull, & Brown trout"))

#This code originates from Lab 9 but from Question 1. Here, I utilize map_int to simulate 10,000 simulations from the randomBabies code using a single input
randomBabies <- function(nBabies = 4){
  baby_tib <- tibble(pnum  = 1:nBabies,
                     bbnum = sample(1:nBabies, 
                                      size = nBabies, 
                                      replace = FALSE)) |>
  mutate(paired = if_else((pnum == bbnum), 
                          "TRUE", 
                          "FALSE")) |>
  filter(paired == "TRUE") |>
  nrow()
  return(baby_tib)}

results <- map_int(.x = 1:10000,
                   .f = ~randomBabies(nBabies = 4))
```

-   `map()` function with **more than one** input (e.g., `map_2()` or `pmap()`)

```{r}
#| label: pe-3-map-2
#Here, i am mapping the function as_factor() at many diffferent locations within the evals df. This is from lab 8, Q2 (but given greater fanciness with lab 9 Challenge)
evals |>
  head() |> #so you don't have a massive output
  map_at(.at = c("teacher_id", 
               "weekday", 
               "academic_degree", 
               "seniority",
               "gender"),
         .f = ~as.factor(.x))|>
  bind_cols()|>
  kable()|>
  kable_classic(lightable_options = "hover")

#I also use p_map() in lab 9 Q6.
all_simulations <- grid |> 
  mutate(simulated_m = pmap(.l = list(n = n,
                                      df = df), 
                            .f = simulate_means)) |> 
  unnest(simulated_m)
```

**PE-4: I can use modern tools when carrying out my analysis.**

-   I can use functions which are not superseded or deprecated

```{r}
#| label: pe-4-1
#There are a few instances in Lab 5 where I used join_by() which is a function that is relatively more stable/non-depreciated as compared to by = c(). Below, i join by a column with different names. I use a join_by that has different column names (license_id == id) and (id == person_id). 
#I also join by columns with the same name. I make sure to use join_by() and include the same column name to make sure that I am confident that this is the column being used. It may seem repetitive but it is to confirm and ensure name is the joining column.
inner_join(person,
           drivers_license,
           join_by(license_id == id)
           ) |>
  filter(str_detect(
         plate_number, "H42W"))|>
  select(name,
         id
         ) |>
  semi_join(gym_names,
            join_by(name == name)) |>
  inner_join(interview,
            join_by(id == person_id)
            )|>
  pull(transcript)

#This is code from Lab 4. Here, i use pivot_wider() and names_prefix() in conjunction, which is a relatively modern approach to what I wanted to do. Instead of using separate functions to pivot and rename the variable, i was able to do it all in one place. 
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

-   I can connect a data wrangling pipeline into a `ggplot()`

```{r}
#| label: pe-4-2
#This code is from Lab 4 question 6. Here, i wrangle data using pivoting, selecting, and mutating. Afterwards, I connect it into a ggplot() function. 
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool"))|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7)) 
```

## Data Simulation & Statisical Models

**DSSM-1: I can simulate data from a *variety* of probability models.**

-   Example 1

```{r}
#| label: dsm-1-1
#This code originates from Lab 9 question 1 and here I write a function to simulate a single randomized combination of babies to their families to see the number of random correct returns. Here, I use sample() as my probability model

randomBabies <- function(nBabies = 4){
  baby_tib <- tibble(pnum  = 1:nBabies,
                     bbnum = sample(1:nBabies, 
                                    size = nBabies, 
                                    replace = FALSE)) |>
  mutate(paired = if_else((pnum == bbnum), 
                          "TRUE", 
                          "FALSE")) |>
  filter(paired == "TRUE") |>
  nrow()
  return(baby_tib)}

results <- map_int(.x = 1:10000,
                   .f = ~randomBabies(nBabies = 4))
```

-   Example 2

```{r}
#| label: dsm-1-2
# This code originates from Lab 9 (the extra questions). Here, I utilize rchisq() and create a function to help me mass simulate using the chi squared probability model. 
simulate_means <- function(n, df){
  map_dbl(.x = 1:n, #simulation amount fixed
          .f = ~rchisq(n = 100, df = df) |> 
          mean()
          )
}
# I then apply it by creating a grid and applying it as a list
grid <- crossing(n = c(10,
                       100,
                       1000,
                       10000), 
                 df = 10) #df was changed after feedback
all_simulations <- grid |> 
  mutate(simulated_m = pmap(.l = list(n = n,
                                      df = df), 
                            .f = simulate_means)) |> 
  unnest(simulated_m) 
```

**DSSM-2: I can conduct common statistical analyses in R.**

-   Example 1

```{r}
#| label: dsm-2-1
#This code is from Lab 3 challenge (last chunk). In here, I run a chi squared test between the seniority level of professors and the SET level average for question 3 (a review of activities in their class). 
chisq.test(teacher_evals_compare$sen_level, 
           teacher_evals_compare$SET_level)
```

-   Example 2

```{r}
#| label: dsm-2-2
#This is from Lab 4 question 8. Here, I use a liner regression model to identify the correlation between median household income and weekly cost of center-based childcare in 2018. I altered this data to avoid creating an intermediate object. 
summary(lm(data = ca_childcare, 
               mc_infant ~ mhi_2018))
```

## Revising My Thinking

I have revised my thinking in various ways throughout the course. I have made revisions to every document that requires alterations. (This includes my Lab 3, a lab that did not qualify for a revision. Nevertheless, I still wanted to explore how I could improve my code and ensure I remember the mistakes by actively fixing them.) This demonstrates my ability to incorporate feedback into my work and produce improved work.Â 

Additionally, my ability to revise my thinking is demonstrated in my revision reflections. It was not enough to simply state the mistake and alterations made, I wanted to make it clear that I have learned from the feedback that was given to me. For example, here is a revision of my code below:

Lab 3 Challenge Revision Reflection

*Reflection: Originally, I neglected to specify that the analysis was specifically about question 3. I forgot that I filtered out for SET question 3. Instead, my conclusion extrapolated beyond the actual findings and made the generalization for ALL questions. This is an important distinction to make because otherwise I am talking about the professorâs evaluation in general as opposed to their use of activities. It is important to make sure that the analysis and the conclusion line up because otherwise I am making a statement backed by incompatible data.*

Another reflection comes from Lab 2 Revision

colnames(surveys) revised to be glimpse(surveys)

*Reflection: I originally just provided the variables in this data set. However, the original question is to provide the data types, not just the variables. The original function I used (colnames()) does not provide any of this information while the new function I utilize (glimpse()) does. It is important to address the question at hand, especially when it has implications for how to run the code. There are many functions that rely on the data being either categorical/characters or numeric (either continuous or discrete). Although just getting familiar with the variable names is important before working with the data, getting a more holistic glimpse of how the data is formatted is far more important.*

Lastly, I have a reflection about showing my work over the product (quality over completion). This reflection comes from my to-be-submitted Lab 5 revision.

From this reflection, I added this code:

right_join(interview,

Â Â  Â  Â  Â  Â  person,

Â Â  Â  Â  Â  Â  join_by(person_id == id))\|\>

Â  select(name,Â 

Â Â  Â  Â  Â  transcript)\|\>

Â  filter(str_detect(name, "Miranda Priestly"))

*Reflection: At the very end of the document, I conclude that Miranda Priestly is the culprit. However, you noted that I did not check the interview for Miranda Priestly. Originally, I opened the data set in a separate tab to check and found nothing from her. This confused me and I assumed that it means it was the end of the lab so I decided to convict her. This, however, was the wrong course of action. It was not justified through code and there is no way I would have known she did not have a transcript if I hadnât had access to the raw datasets. Instead, I should have demonstrated my ability to join and filter datasets. I was discussing this in class and have recently discovered the importance not just taking out information but instead coding it in and justifying it.Â  Sure, I was able to get the correct suspect but it means nothing if I cannot justify it (even down to the very end when checking the interviews). This is important because in a practical setting, it is best to provide your strongest argument and leave no loose ends. Even though I knew there was no transcript, it is important to show that I know it and show I know how to find that information with code.*

Typically half of the text in my reflections were dedicated to directly addressing WHY this fix was important, not only the fact that I made the switch. It is important to get to the root of the issue because that is how you learn & improve for the future.Â 

Even if a reflection was not needed (in the case of a success or ineligibility), I still revised my work and utilized the feedback I received. In particular, I was given comments for every section but had a satisfactory submission such that a revision was not needed. Regardless, I went back and added code such as â+theme(axis.text.x = element_text(size = 7), axis.text.y = element_text(size = 7))â and âtheme_bw()+â. Even though they were little changes, I wanted to take advantage of the advice I was given and solidify in my mind that these are important things to remember.

**Extending My Thinking**

Throughout the quarter, I have tried to extend my thinking when possible. The only time in which we have been given the opportunity to choose medium, spicy or super spicy (Lab 2), I did not choose the simplest option and instead went for medium & spicy. I decided against the super spicy due to my horribly busy schedule but wanted to push myself beyond the basic challenge assignment. I struggled with the spicy section and asked many questions in the discord channel after getting stuck. Nevertheless, I stuck to it and was able to submit it. Additionally, my Lab 3 challenge received all successes and I tried my best to use what I knew to match the graph and preform a chi squared analysis.Â 

Outside of class, I have also utilized the R coding knowledge I have obtained and applied it to my experimental write ups. Below is a line of code I wrote for and included in my Environmental Physiology Report

temp \|\>

Â  ggplot(mapping = aes(y=temp,Â 

Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  x=type))+

Â  geom_boxplot()+

Â  geom_jitter(alpha = .5)+

Â  theme_bw()+

Â  labs(title = "External and Internal Temperatures of the Western Fence Lizard",

Â Â  Â  Â  subtitle = "Y-Axis Represening Temperature (ÂºC)",

Â Â  Â  Â  y = "",

Â Â  Â  Â  x = "Location")

AndÂ 

t.test(temp\$temp \~ temp\$type,

Â Â Â  Â  Â  conf.level=.95)

I hope to one day utilize R code in the studies I anticipate publishing. With the skills I have learned and anticipate building on, I am confident in my abilities to extend my thinking.

**Peer Support & Collaboration**

There are a number of ways throughout the quarter in which I have demonstrated my peer support and collaborative abilities. In class, I will often try to talk to the people next to me and will often refer to them before raising my hand. One of the goals of this is to mitigate the flood questions I typically ask. I try to help those with questions both to strengthen my own code communication skills and potentially learn something new through trial and error. I ask the people around me because they are going through the same processes as me and it may help them too in the case they have the same problem.

Outside of class, I also collaborate with my peers through peer feedback, office hours, and over discord. During office hours, I will often try to talk to other students, involve myself in conversation, and ask questions that may apply to other people. I distinctly remember a few times in which I had already completed a problem but stopped to talk about it with others in order to help them and/or improve my already-formed code.Â 

During peer feedback, I make sure to provide positive feedback with specific examples:

âI love how you used name.x within the pull for âFinding a Suspectâ section. This is was a great use of the .x and is definitely stable formatting that you can use for your portfolio. You did a wonderful job annotating and explaining your thought process. I love the titles and descriptions that you include over each code chunk. It makes your flow of work easier to follow along with and cleaner.â

I also make sure to point out areas of weakness, sometimes ways they hadnât considered:Â 

âYou automatically filtered the name âAnnabel millerâ even though we did not know her last name at that point in time. Make sure that you are showing all of your steps and exactly how you got that last name.â

Occasionally I include fixes to the code Iâve also received criticism on:

âConsider trying to use different functions other than just full and inner joins. This could help with the midterm portfolio (I know I struggled on that so I understand)â

Additionally, Iâve been active on the discord, oftentimes having questions that apply to many people:

âSorry for asking so many questions, this one isn't necessarily about the coding itself. Is it fine that the graph is naturally just a little squashed? everything is there but the numbers at the bottom are real close. I've tried looking for resizing options but we haven't discussed it in class (to my knowledge)â
