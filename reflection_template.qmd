---
title: "STAT 331 Portfolio"
author: "Stella Raymond"
format: 
  html: 
    self-contained: true
layout: margin-left
editor: visual
execute: 
  eval: false
  echo: true
  message: false
---

[**My Grade:**]{.underline} I believe my grade equivalent to course work evidenced below to be an -A.

[**Learning Objective Evidence:**]{.underline} In the code chunks below, provide code from Lab or Challenge assignments where you believe you have demonstrated proficiency with the specified learning target. Be sure to specify **where** the code came from (e.g., Lab 4 Question 2).

## Working with Data

**WD-1: I can import data from a *variety* of formats (e.g., csv, xlsx, txt, etc.).**

-   `csv`

```{r}
#| label: wd-1-csv
#This originated from Lab 2 Challenge (the first load in), the very first load in. I removed library(viridis) becuase that is not relevant here.
library(tidyverse)
library("here")
surveys <- read_csv(here::here("data","surveys.csv"))
```

-   `xlsx`

```{r}
#| label: wd-1-xlsx
#This code originates from the practice activity 4 - Military spending. Here, not only do we load in a xlsx file, we also specify the sheet and rows we wanted to include. 
military <- read_xlsx(here::here("data", 
                                 "gov_spending_per_capita.xlsx"), 
                      sheet = "Share of Govt. spending", 
                      skip  = 7, 
                      n_max = 190)
```

-   `txt`

```{r}
#| label: wd-1-txt
#This code originates from Check-in 2.3. 
ages_tab <- read_table(file = here::here("Week 2", 
                                         "Check-ins", 
                                         "Ages_Data", 
                                         "ages_tab.txt"))
```

**WD-2: I can select necessary columns from a dataset.**

```{r}
#| label: wd-2
#This originated from Lab 4 Q4. Here I select only region, study_year, and mhi_2018 to remain within the dataset)
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

**WD-3: I can filter rows from a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   numeric

```{r}
#| label: wd-3-numeric
#This originated from Lab 4 Q4. Here I filter out year 2008 and 2018.
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

-   character -- specifically a string (example must use functions from **stringr**)

```{r}
#| label: wd-3-string
#This question originates from Lab 5. In this particular chunk, I filter using str_detect so I could extract all Annabels
person |> 
  full_join(interview,
            join_by(id == person_id))|>
  filter(
    (address_street_name == "Northwestern Dr" & address_number == max(address_number)) |
    (address_street_name == "Franklin Ave" & str_detect(name, "^Annabel"))
         )|>
    pull(transcript,
         name)
```

-   factor

```{r}
#| label: wd-3-factor
#i have decided to wait for a future lab in which I can utilize this
```

-   date (example must use functions from **lubridate**)

```{r}
#| label: wd-3-date
#This code comes from Lab 5. Originally, I did not convert the dates into lubridate dates. However, 
crime_scene_report |>
  filter(date == 20180115,
         type == "murder",
         city == "SQL City") |>
  pull(description)

#Here is the updated code USING lubridate instead of filtering it just as a numeric value
crime_scene_report |>
  mutate(date = ymd(date)
         )|>
  filter(date == "2018-01-15",
         type == "murder",
         city == "SQL City") |>
  pull(description)
```

**WD-4: I can modify existing variables and create new variables in a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   numeric (using `as.numeric()` is not sufficient)

```{r}
#| label: wd-4-numeric
#This originates from the set up in Lab 3 Challenge (the first chunk). Here, I am creating a cleaner version of the dataset's question_no variable by subtracting 900 (effectively converting each question number into the a number 1-9 isntead of 901-909).
clean_evals <- evals |>
  mutate(sex = gender,
         teacher_id = as.factor(teacher_id),
         question_no = question_no-900)
```

-   character -- specifically a string (example must use functions from **stringr**)

```{r}
#| label: wd-4-string
#




```

-   factor (example must use functions from **forcats**)

```{r}
#| label: wd-4-factor
#The question is from Lab 4 Q6. In this code chunk, I am changing age into a factor that includes all three age groups. Afterwards, I recode it and change the names.
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool"))
```

-   date (example must use functions from **lubridate**)

```{r}
#| label: wd-4-date
#As mentioned in WD-3, this line of code comes from Lab 5 and is altered from my original submission to include a date mutation with lubridate using ymd()
crime_scene_report |>
  mutate(date = ymd(date)
         )|>
  filter(date == "2018-01-15",
         type == "murder",
         city == "SQL City") |>
  pull(description)
```

**WD-5: I can use mutating joins to combine multiple dataframes.**

-   `left_join()`

```{r}
#| label: wd-5-left
#This code is from Lab 4 question 2. Here I implement a left_join()

ca_childcare <- counties |>
  filter(state_abbreviation == "CA") |>
  left_join(childcare_costs,
            by = join_by(county_fips_code == county_fips_code))
```

-   `right_join()`

```{r}
#| label: wd-5-right
#This is from Lab 5, a portion of the lab I am currently revising. Here, I confirm that the suspect is the true murderer by joining the two dataframes and pulling her name.
right_join(interview,
           person,
           join_by(person_id == id))|>
  select(name, 
         transcript)|>
  filter(str_detect(name, "Miranda Priestly"))
```

-   `inner_join()`

```{r}
#| label: wd-5-inner
#Here i include code from Lab 5 and combine three different dataframes. I also use robust & resitant coding by confirming that ssn == ssn just in case. 
inner_join(person,
           drivers_license,
           join_by(license_id == id)) |>
  inner_join(facebook_event_checkin,
           join_by(id == person_id)) |>
  inner_join(income, join_by(ssn == ssn))
```

-   `full_join()`

```{r}
#| label: wd-5-full
#This code originates from lab 5 where I have altered my original code. Here, I utilize a full_join() to incorpreate the data from the interview dataframe in order to go straight from the 2 witnesses to the transcripts.
person |> 
  full_join(interview,
            join_by(id == person_id))|>
  filter(
    (address_street_name == "Northwestern Dr" & address_number == max(address_number)) |
    (address_street_name == "Franklin Ave" & str_detect(name, "Annabel"))
         )|>
    pull(transcript,
         name)
```

**WD-6: I can use filtering joins to filter rows from a dataframe.**

-   `semi_join()`

```{r}
#| label: wd-6-semi
#This code is from lab 5 and is a part of the revision I am making. Here, I utilize a semi_join near the end of my code chunk
gym_names <- inner_join(get_fit_now_member, 
                        get_fit_now_check_in,
                        by = join_by(id == membership_id)
                        )|>
  select(name,
         id,
         membership_status,
         check_in_date
         )|>
  filter(str_detect(id, "^48Z"),
         membership_status == "gold")

inner_join(person,
           drivers_license,
           join_by(license_id == id)
           ) |>
  filter(str_detect(
         plate_number, "H42W"))|>
  select(name,
         id
         ) |>
  semi_join(gym_names,
            join_by(name == name)
            )|>
  inner_join(interview,
            join_by(id == person_id)
            )|>
  pull(transcript)
```

-   `anti_join()`

```{r}
#| label: wd-6-anti
#I chose to wait for a future lab in which i can utilize this function
```

**WD-7: I can pivot dataframes from long to wide and visa versa**

-   `pivot_longer()`

```{r}
#| label: wd-7-long
#This code originates from a small portion of Lab 4's Question 6. Here I take the three separate age colomns and combine them into a single colomn.
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price")
```

-   `pivot_wider()`

```{r}
#| label: wd-7-wide
#This is from Lab 4 question 4. Here, I make two new colomns out of the two years from study_year.
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

## Reproducibility

**R-1: I can create professional looking, reproducible analyses using RStudio projects, Quarto documents, and the here package.**

I've done this in the following provided assignments:

-   Lab 3 Challenge

-   Lab 4

-   Lab 5

**R-2: I can write well documented and tidy code.**

-   Example of **ggplot2** plotting

```{r}
#| label: r-2-1
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool"))|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7))
```

-   Example of **dplyr** pipeline

```{r}
#| label: r-2-2
#This code chunk is from Lab 4 question 3. I chose to include this code chunk because it was comprised of a large amount of characters and was thus difficult to deal with. Nevertheless, I was able to keep the code tidy and organized.
ca_childcare <- ca_childcare |> 
  mutate(county_name = str_remove(county_name, 
                                  " County"),
         region = fct_collapse(.f = county_name,
                               "Superior California" = c("Butte", 
                                                         "Colusa", 
                                                         "El Dorado", 
                                                         "Glenn", 
                                                         "Lassen", 
                                                         "Modoc", "Nevada", 
                                                         "Plumas", 
                                                         "Sacramento", 
                                                         "Shasta", 
                                                         "Sierra", 
                                                         "Siskiyou", 
                                                         "Sutter", 
                                                         "Tehama", 
                                                         "Yolo", 
                                                         "Yuba", 
                                                         "Placer"),
                               "San Francisco Bay Area" = c("Alameda", 
                                                            "Contra Costa", 
                                                            "Marin", 
                                                            "San Francisco", 
                                                            "San Mateo", 
                                                            "Santa Clara", 
                                                            "Solano"),
                               "North Coast" = c("Del Norte", 
                                                 "Humboldt", 
                                                 "Lake", 
                                                 "Mendocino", 
                                                 "Napa", 
                                                 "Sonoma", 
                                                 "Trinity"),
                               "Orange County" = c("Orange"),
                               "Central Coast" = c("Monterey", 
                                                   "San Benito", 
                                                   "San Luis Obispo", 
                                                   "Santa Barbara", 
                                                   "Santa Cruz", 
                                                   "Ventura"),
                               "Northern San Joaquin Valley" = c("Alpine", 
                                                                 "Amador", 
                                                                 "Calaveras", 
                                                                 "Madera", 
                                                                 "Mariposa", 
                                                                 "Merced", 
                                                                 "Mono", 
                                                                 "San Joaquin", 
                                                                 "Stanislaus", 
                                                                 "Tuolumne"),
                               "Los Angeles County" = c("Los Angeles"),
                               "Southern San Joaquin Valley" = c("Fresno", 
                                                                 "Kern", 
                                                                 "Kings", 
                                                                 "Inyo", 
                                                                 "Tulare"),
                               "Inland Empire" = c("Riverside", 
                                                   "San Bernardino"),
                               "San Diego Imperial" = c("San Diego", 
                                                       "Imperial")))
```

-   Example of function formatting

```{r}
#| label: r-2-3
#*

rescale_01 <- function(x, na.rm = TRUE){
  if(!is.numeric(x) | length(x) <= 1)
  {stop("change input")}
  ranging <- range(x, 
                   na.rm = na.rm)
  return((x - ranging[1]) / 
           (ranging[2] - ranging[1]))}
```

**R-3: I can write robust programs that are resistant to changes in inputs.**

-   Example -- any context

```{r}
#| label: r-3-example
#Here is a line of code from Lab 4 question 5. In this, i utilize a robust means of extracting the minimum median region value using slice_min. Instead of selecting a certain row, i request that the minimum value is extracted, ensuring that regardless of how shuffled the dataset is, i get the exact value i want. 
ca_childcare |>
  filter(study_year == "2018") |>
  select(region, 
         study_year, 
         mcsa) |>
  group_by(region)|>
  summarise(median_region = median(mcsa))|>
  slice_min(median_region)

#Additionally, I also use summarise in the context of multiple columns. This code originates from Lab 7

fish |>
  summarize(across(.cols = trip:species,
                   .fns = ~sum(is.na(.x)
                               )
                   )
            )
```

-   Example of function stops

```{r}
#| label: r-3-function-stops
#This code originates from lab 7 and it is a post-revision line of code. Here I use a function stop to ensure that the input is both numeric and ensure that the input is not greater than 1. 

rescale_01 <- function(x, na.rm = TRUE){
  if(!is.numeric(x) | length(x) <= 1)
  {stop("change input")}
  ranging <- range(x, 
                   na.rm = na.rm)
  return((x - ranging[1]) / 
           (ranging[2] - ranging[1]))}
```

## Data Visualization & Summarization

**DVS-1: I can create visualizations for a *variety* of variable types (e.g., numeric, character, factor, date)**

-   at least two numeric variables

```{r}
#| label: dvs-1-num
#This is from Lab 4 Question 6. Here, the two numeric variables are year and cost of child support. 
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool"))|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)))+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7)) 
```

-   at least one numeric variable and one categorical variable

```{r}
#| label: dvs-2-num-cat
#This code originates from my Lab 2 challenge (the third chunk). (Here I deleted an extra comma but other than that, should be good). Numeric: wight Cat: species
ggplot(data = surveys,
       mapping = aes(
         x= weight, 
         y= species)) +
  geom_jitter(alpha=0.5, 
              color="green4")+
  geom_density_ridges(alpha=0.5)+
  labs(
    x="Weight (g)", 
    y="", 
    title= "Relationship Between Rodent Weight and Hindfoot Length",
    subtitle= "Y-axis representing Species")
```

-   at least two categorical variables

```{r}
#| label: dvs-2-cat
#This is from Lab 2 Challenge (the 4th and 5th chunks). Here I create a graph using the vector color pallete "Palette_Earth" I created. The two categorical variables are Genus and Species. 

Palette_Earth <- c("#a0eb67", "#83ba59", "#3e6b1b", "#78eb05", "#ffd673", "#c99f3a", "#8a6204", "#d9cb0f")

surveys |> 
  ggplot(aes(x = weight, 
             y = species, 
             color = genus)) +
  geom_boxplot() +
  scale_color_manual(values = Palette_Earth) +
  labs(x = "Weight (g)", 
       y = "", 
       subtitle = "Species", 
       legend = "Genus")
```

-   dates (timeseries plot)

```{r}
#| label: dvs-2-date

CHECK?
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool")
         )|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()
                     ) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7))
```

**DVS-2: I use plot modifications to make my visualization clear to the reader.**

-   I can ensure people don't tilt their head

```{r}
#| label: dvs-2-1
#Here is a line of code from Lab 2 question 16. I implement two ways of increasing readiblity: altering species to the y-axis and using the subtitle in place of a y-axis label.
ggplot(data = surveys,
       mapping = aes(
         x = weight, 
         y = species
         )
       ) +
  geom_jitter(alpha = 0.5, 
              color = "green4")+
  geom_boxplot(outliers = FALSE)+
  labs(
    x = "Weight (g)", 
    y = "", 
    title = "Relationship Between Rodent Weight and Hindfoot Length",
    subtitle = "Y-axis representing Species",)
```

-   I can modify the text in my plot to be more readable

```{r}
#| label: dvs-2-2
#This is from lab 4 question 6. Here, I make the plot more readible through various means. In particular, I alter the size of the axes, the label names, and the order of the legend.
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool")
         )|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()
                     ) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7)) 
```

-   I can reorder my legend to align with the colors in my plot

```{r}
#| label: dvs-2-3
#Again, this code is from lab 4 question 6. Here, I reorder the legend color data in order to match with the lines
ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool")
         )|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()
                     ) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7)) 
```

**DVS-3: I show creativity in my visualizations**

-   I can use non-standard colors

```{r}
#| label: dvs-3-1
#I used non standard colors in Lab 2 challenge (7th and 8th chunks) by using colors from packages ggsci and viridis
surveys |> 
  ggplot(aes(x = weight, y = species, color = genus)
         ) +
  geom_boxplot() +
  scale_colour_viridis_d() +
  labs(x = "Weight (g)", y = "", subtitle = "Species", legend = "Genus")

#this second code chunk is for a separate graph with alternative colors
surveys |> 
  ggplot(aes(x = weight, y = species, color = genus)
         ) +
  geom_boxplot() +
  scale_color_lancet() +
  labs(x = "Weight (g)", y = "", subtitle = "Species", legend = "Genus")
```

-   I can use annotations

```{r}
#| label: dvs-3-2
#This code is from lab 3 challenge (question 2) and I have updated it such that I included annotations in the graph to better visualize which section of the stacked bar graph is which. This was not included in the revisions but added here.
teacher_evals_compare |>
ggplot(mapping=aes(x = sen_level, 
                   fill = SET_level)) +
  
  theme_bw()+
  scale_fill_manual(values = c("#2d73b5",
                               "#c97a0a")) +
  geom_bar() +
  annotate("text", 
           y = 50,
           x = "senior", 
           label = "Standard", 
           color = "white") +
  annotate("text", 
           y = 250, 
           x = "senior", 
           label = "Excellent", 
           color = "white") +
  annotate("text", 
           y = 25, 
           x = "junior", 
           label = "Standard", 
           color = "white") +
  annotate("text", 
           y = 175, 
           x = "junior", 
           label = "Excellent", 
           color = "white") +
  labs(title = "Evaluation of In-Class Activity Use by Instructor Seniority",
       x="Seniority of Instructor",
       y="",
       subtitle = "Number of Sections",
       fill = "SET Level")
```

-   I can be creative...

```{r}
#| label: dvs-3-3
#This code originates from Lab 4 Question 7. I will admit, I did not follow the question completely: I extended past what was asked for and included regional data when i wasn't supposed to. I would like to argue that this shows my creativity. I was originally just playing around with the graph and wanted to explore some trends. I was curious how region played into this correlation and was interested in the results of this inclusion (for examoke, all San Fransisco values are very high, Central Coast are right in the middle, etc.). For this reason, adding color just as an extra layer to graph demonstrates that I am thinking about what researchers would want to present and creative ways to include notible finds. 
ca_childcare |>
  filter(study_year == "2018") |>
  select(mhi_2018,
         mc_infant,
         study_year,
         region) |>
  ggplot(mapping = aes(x = mhi_2018, 
                       y = mc_infant,
                       color = region)
         )+
  geom_jitter()+
  geom_smooth(method = lm, 
              color = "green4")+
  theme_bw()+
  labs(title = "Household Income and Weekly Price of 2018 Center-Based Infant Childcare",
       x = "Median Weekly Household Income ($)",
       y = "",
       subtitle = "Y-axis representing Full-Time Median Weekly Price ($)",
       color = "California Region")

#Additionally, I chose to show my creativity in my Lab 9 challenge when changing up the tables. As a result, I added percentages, changed color to green (my favorite color I commonly use), utilized subtitle to specify my table, and added table striping. 

baby_table <- enframe(results, 
        name = "simulation_number", 
        value = "ncorrect") |>
  group_by(ncorrect) |>
  count() |>
  ungroup() |>
  mutate(proportion = n/sum(n)) |>
  select(ncorrect,
         proportion)

baby_table |>
  rename("correct returns" = ncorrect) |>
  gt() |>
  fmt_percent(proportion) |>
  tab_header(
    title = "Proportion of Correct Baby Returns",
    subtitle = "Based on 10,000 Randomized Simulations") |>
    #subtitle altered after feedback
  opt_stylize(color = "green", 
              add_row_striping = TRUE)

```

**DVS-4: I can calculate numerical summaries of variables.**

-   Example using `summarize()`

```{r}
#| label: dvs-4-summarize
#Here is an example from Lab 4 question 4 where I utilize summarise to create a new median_region variable
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

-   Example using `across()`

```{r}
#| label: dvs-4-across
# 

rescale_column <- function(df, name)
{df |> 
    mutate(across(.cols = {{name}},
                  ~ rescale_01(.x)
                  )
           )
  }
```

**DVS-5: I can find summaries of variables across multiple groups.**

-   Example 1

```{r}
#| label: dvs-5-1
#The code originates from Lab 4 question 4. Here, I am able to summarize the median region household income based on 2018 values. In this chunk, i group by the two groups region and study_year.
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

-   Example 2

```{r}
#| label: dvs-5-2
#Additionally, I've used the group_by() function to creatue an attendance variable by utilizing the event_name and id variables. This code originates from my updated Lab 5 revisions.

inner_join(person,
           drivers_license,
           join_by(license_id == id)) |>
  inner_join(facebook_event_checkin,
           join_by(id == person_id)) |>
  inner_join(income, join_by(ssn == ssn)) |>
  mutate(comparative_income = mean(annual_income)) |>
  group_by(id, event_name) |>
  mutate(attendance = n()) |>
  ungroup()
```

**DVS-6: I can create tables which make my summaries clear to the reader.**

-   Example 1

```{r}
#| label: dvs-6-1
#This is from Lab 4. I actually fixed this piece of code to better explain my data. Originally I had "mhi_in" as a prefix but altered it to "median_household_income_" to better explain the data.
ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "median_household_income_")|>
  ungroup()

#Additionally, I could alter my code from Lab 3 Challenge such that SET and sen are more clear. Below is an alteration I made to do so. Here is the altered code:

teacher_evals_compare <- clean_evals |>
  filter(question_no == 3) |>
  mutate(SET_Q3_score_average = if_else(SET_score_avg >= 4,
                            "excellent",
                            "standard"),
         seniority_level = if_else(seniority <= 4,
                             "junior",
                             "senior"))|>
  select(course_id,
         SET_Q3_score_average,
         seniority_level)

teacher_evals_compare

#I changed SET_level -> SET_Q3_score_average and sen_level -> seniority_level
```

-   Example 2

```{r}
#| label: dvs-6-2
#This code originates from Lab 9 challenge (so, reformated Lab 8). Here, I utilize gt() to reformat the table and create informative titles/subtitles. I also retroactively added a cool new color to the title and subtitle. Here, the count for each of the columns are clearly aligned on the table and striped for easier viewing.

tibble(Parameters = names(fish), 
       Count = map_int(fish,
                       ~sum(is.na(.x))
                       )
       )|> 
  gt()|>
  tab_header(title = md("Blackfoot River Trout Absent Values"),
             subtitle = md("Rainbow, Westslope Cutthroat, Bull, & Brown trout"))|>
  tab_style(style = cell_fill(color = "darkseagreen3"),
            locations = cells_title(groups = c("title")))|>
  tab_style(style = cell_fill(color = "darkseagreen1"),
            locations = cells_title(groups = c("subtitle")))
```

**DVS-7: I show creativity in my tables.**

-   Example 1

```{r}
#| label: dvs-7-1

```

-   Example 2

```{r}
#| label: dvs-7-2
#*
tibble(Parameters = names(fish), 
       Count = map_int(fish,
                       ~sum(is.na(.x))
                       )
       )|> 
  gt()|>
  tab_header(title = md("Blackfoot River Trout Absent Values"),
             subtitle = md("Rainbow, Westslope Cutthroat, Bull, & Brown trout"))|>
  tab_style(style = cell_fill(color = "darkseagreen3"),
            locations = cells_title(groups = c("title")))|>
  tab_style(style = cell_fill(color = "darkseagreen1"),
            locations = cells_title(groups = c("subtitle")))
```

## Program Efficiency

**PE-1: I can write concise code which does not repeat itself.**

-   using a single function call with multiple inputs (rather than multiple function calls)

```{r}
#| label: pe-1-one-call
#Here is a code chunk from Lab 3 Challenge set up. Here, I use one single mutate function with three inputs (sex, teacher_id, and question_no changes) instead of three separate mutate functions. 
clean_evals <- evals |>
  mutate(sex = gender,
         teacher_id = as.factor(teacher_id),
         question_no = question_no-900)
```

-   `across()`

```{r}
#| label: pe-1-across
#*
rescale_column <- function(df, name)
{df |> 
    mutate(across(.cols = {{name}},
                  ~ rescale_01(.x)))
  }
```

-   `map()` functions

```{r}
#| label: pe-1-map-1

#*

all_simulations <- grid |> 
  mutate(simulated_m = pmap(.l = list(n = n,
                                      df = df), 
                            .f = simulate_means)) |> 
  unnest(simulated_m)

#

randomBabies <- function(nBabies = 4){
  baby_tib <- tibble(pnum  = 1:nBabies,
                     bbnum = sample(1:nBabies, 
                                      size = nBabies, 
                                      replace = FALSE)) |>
  mutate(paired = if_else((pnum == bbnum), 
                          "TRUE", 
                          "FALSE")) |>
  filter(paired == "TRUE") |>
  nrow()
  return(baby_tib)}

results <- map_int(.x = 1:10000,
                   .f = ~randomBabies(nBabies = 4))

#

evals |>
  head() |> #so you don't have a massive output
  map_at(.at = c("teacher_id", 
               "weekday", 
               "academic_degree", 
               "seniority",
               "gender"),
         .f = ~as.factor(.x))|>
  bind_cols()|>
  kable()|>
  kable_classic(lightable_options = "hover")
```

**PE-2: I can write functions to reduce repetition in my code.**

-   Function that operates on vectors

```{r}
#| label: pe-2-1
#This is from Lab 2 Challenge (chunk 1). Here I create a vector to house the colors I wanted for my graph. then I use the vector to color the graph
Palette_Earth <- c("#a0eb67", "#83ba59", "#3e6b1b", "#78eb05", "#ffd673", "#c99f3a", "#8a6204", "#d9cb0f")

surveys |> 
  ggplot(aes(x = weight, y = species, color = genus)
         ) +
  geom_boxplot() +
  scale_color_manual(values = Palette_Earth) +
  labs(x = "Weight (g)", y = "", subtitle = "Species", legend = "Genus")
```

-   Function that operates on data frames

```{r}
#| label: pe-2-2

```

**PE-3:I can use iteration to reduce repetition in my code.**

-   `across()`

```{r}
#| label: pe-3-across

```

-   `map()` function with **one** input (e.g., `map()`, `map_chr()`, `map_dbl()`, etc.)

```{r}
#| label: pe-3-map-1
#*

tibble(Parameters = names(fish), 
       Count = map_int(fish,
                       ~sum(is.na(.x))))|> 
  gt()|>
  tab_header(title = md("Blackfoot River Trout Absent Values"),
             subtitle = md("Rainbow, Westslope Cutthroat, Bull, & Brown trout"))
```

-   `map()` function with **more than one** input (e.g., `map_2()` or `pmap()`)

```{r}
#| label: pe-3-map-2
#* 

evals |>
  head() |> #so you don't have a massive output
  map_at(.at = c("teacher_id", 
               "weekday", 
               "academic_degree", 
               "seniority",
               "gender"),
         .f = ~as.factor(.x))
```

**PE-4: I can use modern tools when carrying out my analysis.**

-   I can use functions which are not superseded or deprecated

```{r}
#| label: pe-4-1

#*??






#This is code from Lab 4. Here, i use pivot_wider() and names_prefix() in conjunction, which is a relatively modern approach to what I wanted to do. Instead of using separate functions to pivot and rename the variable, i was able to do it all in one place. 

ca_childcare |>
  filter(study_year %in% c("2018", 
                           "2008")) |>
  select(region, 
         study_year, 
         mhi_2018) |>
  group_by(region, study_year) |>
  summarise(median_region = median(mhi_2018)) |>
  pivot_wider(id_cols = region,
             names_from = study_year, 
             values_from = median_region,
             names_prefix = "mhi_in_")|>
  ungroup()
```

-   I can connect a data wrangling pipeline into a `ggplot()`

```{r}
#| label: pe-4-2
#This code is from Lab 4 question 6. Here, i wrangle data using pivoting, selecting, and mutating. Afterwards, I connect it into a ggplot() function. 

ca_childcare |>
  select(region, 
         study_year, 
         mc_infant,
         mc_toddler,
         mc_preschool) |>
  pivot_longer(cols = mc_infant:mc_preschool,
               names_to = "age",
               values_to = "price") |>
  mutate(age = fct(age,
                   levels = c("mc_infant",
                              "mc_toddler",
                              "mc_preschool")
                   ),
         age = fct_recode(.f = age,
                    "Infant" = "mc_infant",
                    "Toddler" = "mc_toddler",
                    "Preschool" = "mc_preschool"))|>
  ggplot(mapping = aes(x = study_year,
                       y = price,
                       color = fct_reorder2(.f = region,
                                           .x = study_year,
                                           .y = price)
                       )
         )+
  geom_smooth() +
  geom_point() +
  facet_wrap(~age) +
  scale_x_continuous(n.breaks = 6) +
  scale_y_continuous(n.breaks = 5, labels = label_dollar()) +
  theme_bw() +
  labs(x = "Study Year",
       y = "",
       title = "Weekly Median Price for Center-Based Childcare ($)",
       color = "California Region") +
  theme(axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7)) 
```

## Data Simulation & Statisical Models

**DSSM-1: I can simulate data from a *variety* of probability models.**

-   Example 1

```{r}
#| label: dsm-1-1
#*

randomBabies <- function(nBabies = 4){
  baby_tib <- tibble(pnum  = 1:nBabies,
                     bbnum = sample(1:nBabies, 
                                      size = nBabies, 
                                      replace = FALSE)) |>
  mutate(paired = if_else((pnum == bbnum), 
                          "TRUE", 
                          "FALSE")) |>
  filter(paired == "TRUE") |>
  nrow()
  return(baby_tib)}

results <- map_int(.x = 1:10000,
                   .f = ~randomBabies(nBabies = 4))
```

-   Example 2

```{r}
#| label: dsm-1-2
# This code originates from Lab 9 (the extra questions). Here, I utilize rchisq() and create a function to help me mass simulate using the chi squared probability model. 
simulate_means <- function(n, df){
  map_dbl(.x = 1:100, #*****
          .f = ~rchisq(n = n, df = df) |> 
          mean()
          )
}
# I then apply it by creating a grid and applying it as a list

grid <- crossing(n = c(10,
                       100,
                       1000,
                       10000), 
                 df = 10)
#df changed after feedback
all_simulations <- grid |> 
  mutate(simulated_m = pmap(.l = list(n = n,
                                      df = df), 
                            .f = simulate_means)) |> 
  unnest(simulated_m) 
```

**DSSM-2: I can conduct common statistical analyses in R.**

-   Example 1

```{r}
#| label: dsm-2-1
#This code is from Lab 3 challenge (last chunk). In here, I run a chi squared test between the seniority level of professors and the SET level average for question 3 (a review of activities in their class). 
chisq.test(teacher_evals_compare$sen_level, 
           teacher_evals_compare$SET_level)
```

-   Example 2

```{r}
#| label: dsm-2-2
#This is from Lab 4 question 8. Here, I use a liner regression model to identify the correlation between median household income and weekly cost of center-based childcare in 2018. I altered this data to avoid creating an intermediate object. 
summary(lm(data = ca_childcare, 
               mc_infant ~ mhi_2018))
```

## Revising My Thinking

I have revised my thinking in various ways throughout the course. I have made revisions to every document that requires alterations. (This includes my Lab 3, a lab that did not qualify for a revision. Nevertheless, I still wanted to explore how I could improve my code and ensure I remember the mistakes by actively fixing them.) This demonstrates my ability to incorporate feedback into my work and produce improved work. 

Additionally, my ability to revise my thinking is demonstrated in my revision reflections. It was not enough to simply state the mistake and alterations made, I wanted to make it clear that I have learned from the feedback that was given to me. For example, here is a revision of my code below:

Lab 3 Challenge Revision Reflection

*Reflection: Originally, I neglected to specify that the analysis was specifically about question 3. I forgot that I filtered out for SET question 3. Instead, my conclusion extrapolated beyond the actual findings and made the generalization for ALL questions. This is an important distinction to make because otherwise I am talking about the professor’s evaluation in general as opposed to their use of activities. It is important to make sure that the analysis and the conclusion line up because otherwise I am making a statement backed by incompatible data.*

Another reflection comes from Lab 2 Revision

colnames(surveys) revised to be glimpse(surveys)

*Reflection: I originally just provided the variables in this data set. However, the original question is to provide the data types, not just the variables. The original function I used (colnames()) does not provide any of this information while the new function I utilize (glimpse()) does. It is important to address the question at hand, especially when it has implications for how to run the code. There are many functions that rely on the data being either categorical/characters or numeric (either continuous or discrete). Although just getting familiar with the variable names is important before working with the data, getting a more holistic glimpse of how the data is formatted is far more important.*

Lastly, I have a reflection about showing my work over the product (quality over completion). This reflection comes from my to-be-submitted Lab 5 revision.

From this reflection, I added this code:

right_join(interview,

           person,

           join_by(person_id == id))\|\>

  select(name, 

         transcript)\|\>

  filter(str_detect(name, "Miranda Priestly"))

*Reflection: At the very end of the document, I conclude that Miranda Priestly is the culprit. However, you noted that I did not check the interview for Miranda Priestly. Originally, I opened the data set in a separate tab to check and found nothing from her. This confused me and I assumed that it means it was the end of the lab so I decided to convict her. This, however, was the wrong course of action. It was not justified through code and there is no way I would have known she did not have a transcript if I hadn’t had access to the raw datasets. Instead, I should have demonstrated my ability to join and filter datasets. I was discussing this in class and have recently discovered the importance not just taking out information but instead coding it in and justifying it.  Sure, I was able to get the correct suspect but it means nothing if I cannot justify it (even down to the very end when checking the interviews). This is important because in a practical setting, it is best to provide your strongest argument and leave no loose ends. Even though I knew there was no transcript, it is important to show that I know it and show I know how to find that information with code.*

Typically half of the text in my reflections were dedicated to directly addressing WHY this fix was important, not only the fact that I made the switch. It is important to get to the root of the issue because that is how you learn & improve for the future. 

Even if a reflection was not needed (in the case of a success or ineligibility), I still revised my work and utilized the feedback I received. In particular, I was given comments for every section but had a satisfactory submission such that a revision was not needed. Regardless, I went back and added code such as “+theme(axis.text.x = element_text(size = 7), axis.text.y = element_text(size = 7))” and “theme_bw()+”. Even though they were little changes, I wanted to take advantage of the advice I was given and solidify in my mind that these are important things to remember.

**Extending My Thinking**

Throughout the quarter, I have tried to extend my thinking when possible. The only time in which we have been given the opportunity to choose medium, spicy or super spicy (Lab 2), I did not choose the simplest option and instead went for medium & spicy. I decided against the super spicy due to my horribly busy schedule but wanted to push myself beyond the basic challenge assignment. I struggled with the spicy section and asked many questions in the discord channel after getting stuck. Nevertheless, I stuck to it and was able to submit it. Additionally, my Lab 3 challenge received all successes and I tried my best to use what I knew to match the graph and preform a chi squared analysis. 

Outside of class, I have also utilized the R coding knowledge I have obtained and applied it to my experimental write ups. Below is a line of code I wrote for and included in my Environmental Physiology Report

temp \|\>

  ggplot(mapping = aes(y=temp, 

                       x=type))+

  geom_boxplot()+

  geom_jitter(alpha = .5)+

  theme_bw()+

  labs(title = "External and Internal Temperatures of the Western Fence Lizard",

       subtitle = "Y-Axis Represening Temperature (ºC)",

       y = "",

       x = "Location")

And 

t.test(temp\$temp \~ temp\$type,

        conf.level=.95)

I hope to one day utilize R code in the studies I anticipate publishing. With the skills I have learned and anticipate building on, I am confident in my abilities to extend my thinking.

**Peer Support & Collaboration**

There are a number of ways throughout the quarter in which I have demonstrated my peer support and collaborative abilities. In class, I will often try to talk to the people next to me and will often refer to them before raising my hand. One of the goals of this is to mitigate the flood questions I typically ask. I try to help those with questions both to strengthen my own code communication skills and potentially learn something new through trial and error. I ask the people around me because they are going through the same processes as me and it may help them too in the case they have the same problem.

Outside of class, I also collaborate with my peers through peer feedback, office hours, and over discord. During office hours, I will often try to talk to other students, involve myself in conversation, and ask questions that may apply to other people. I distinctly remember a few times in which I had already completed a problem but stopped to talk about it with others in order to help them and/or improve my already-formed code. 

During peer feedback, I make sure to provide positive feedback with specific examples:

“I love how you used name.x within the pull for “Finding a Suspect” section. This is was a great use of the .x and is definitely stable formatting that you can use for your portfolio. You did a wonderful job annotating and explaining your thought process. I love the titles and descriptions that you include over each code chunk. It makes your flow of work easier to follow along with and cleaner.”

I also make sure to point out areas of weakness, sometimes ways they hadn’t considered: 

“You automatically filtered the name “Annabel miller” even though we did not know her last name at that point in time. Make sure that you are showing all of your steps and exactly how you got that last name.”

Occasionally I include fixes to the code I’ve also received criticism on:

“Consider trying to use different functions other than just full and inner joins. This could help with the midterm portfolio (I know I struggled on that so I understand)”

Additionally, I’ve been active on the discord, oftentimes having questions that apply to many people:

“Sorry for asking so many questions, this one isn't necessarily about the coding itself. Is it fine that the graph is naturally just a little squashed? everything is there but the numbers at the bottom are real close. I've tried looking for resizing options but we haven't discussed it in class (to my knowledge)”
